\documentclass[a4paper,12pt]{article}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{titlesec}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{array}
\usetikzlibrary{positioning,arrows.meta} % Load the positioning library

\usetikzlibrary{shadows}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{lipsum}
\usepackage{mdframed}
\usepackage{pagecolor}
\usepackage{mathpazo}   % Palatino font (serif)
\usepackage{microtype}  % Better typography

% Page background color
\pagecolor{gray!10!white}

% Geometry settings
\geometry{margin=0.5in}
\pagestyle{fancy}
\fancyhf{}

% Fancy header and footer
\fancyhead[C]{\textbf{\color{blue!80}CS726 Programming Assignment -- 3 Report}}
\fancyhead[R]{\color{blue!80}Bayesian Bunch}
\fancyfoot[C]{\thepage}

% Custom Section Color and Format with Sans-serif font
\titleformat{\section}
{\sffamily\color{purple!90!black}\normalfont\Large\bfseries}
{\thesection}{1em}{}

% Custom subsection format
\titleformat{\subsection}
{\sffamily\color{cyan!80!black}\normalfont\large\bfseries}
{\thesubsection}{1em}{}

% Stylish Title with TikZ (Enhanced with gradient)
\newcommand{\cooltitle}[1]{%
  \begin{tikzpicture}
    \node[fill=blue!20,rounded corners=10pt,inner sep=12pt, drop shadow, top color=blue!50, bottom color=blue!30] (box)
    {\Huge \bfseries \color{black} #1};
  \end{tikzpicture}
}
\usepackage{float} % Add this package

\newenvironment{solution}[2][]{%
    \begin{mdframed}[linecolor=blue!70!black, linewidth=2pt, roundcorner=10pt, backgroundcolor=yellow!10!white, skipabove=12pt, skipbelow=12pt]%
        \textbf{\large #2}
        \par\noindent\rule{\textwidth}{0.4pt}
}{
    \end{mdframed}
}

% Document title
\title{\cooltitle{CS726 Programming Assignment -- 3 Report}}
\author{
\textbf{Saksham Rathi (22B1003)}\\
\textbf{Sharvanee Sonawane (22B0943)}\\
\textbf{Deeksha Dhiwakar (22B0988)}\\
\small Department of Computer Science, \\
Indian Institute of Technology Bombay \\}
\date{}

\begin{document}
\maketitle

\section{Task 0: Introduction to LLM Decoding Techniques}

Here is how, we are getting the logits from the model. To speed up the process, we are also using cache, which stores the previous model run, and passes it for the next run. This speeds up the process by roughly 5 times. 
\begin{verbatim}
  for i in range(self.max_output_len):
    outputs = self.model(
        input_ids=current_ids,
        past_key_values=past_key_values,
        use_cache=True
    )
    logits = outputs.logits
    past_key_values = outputs.past_key_values
    logit_last_token = logits[:, -1, :]
\end{verbatim}

\subsection{Greedy Decoding}
Here is how greedy decoding is implemented:
\begin{verbatim}
  next_token = torch.argmax(logit_last_token, dim=-1)
\end{verbatim}

That is, we are taking the token corresponding to the maximum probability (dim = -1 stores the probabilities across the vocabulary). 


Here are some sample runs:

\begin{verbatim}
Example: 1/50

Reference: an appearance is a bunch of attributes related to the service person like their 
shoes clothes tie jewellery hairstyle makeup watch cosmetics perfume etc

Ground Truth: service is the combination of many qualities for people such as their 
clothes shoes ties accessories makeup hairstyle cosmetics etc


Example: 10/50

Reference: send rama with the sage and send lakshmana too

Ground Truth: ram with the sage and lakshman also send
\end{verbatim}
Here are the final score values:

\begin{verbatim}
BLEU: 0.31440443213296393
ROUGE-1: 0.3571874622955566
ROUGE-2: 0.13222295518311183
ROUGE-LCS: 0.27441622904852214
\end{verbatim}


\subsection{Random Sampling with Temperature Scaling}
Here is how random sampling is implemented with temperature $\tau$:
\begin{verbatim}
  next_token_logits = next_token_logits / self.tau
  probs = torch.softmax(next_token_logits, dim=-1)
  next_token = torch.multinomial(probs, num_samples=1)
\end{verbatim}


\subsubsection{$\tau = 0.5$}
Sample run:
\begin{verbatim}
Example: 11/50

Reference: but mangal pandeys brave deed was done through devotion to a high and 
noble principle

Ground Truth: parantu mangal pande ne yah saahsik kaarnama aeek unche aur shreshtha
siddhant ke pratip sampan ke liye kiya
\end{verbatim}
Scores:
\begin{verbatim}
BLEU: 0.2838258164852255
ROUGE-1: 0.28984430881105616
ROUGE-2: 0.10543762417930613
ROUGE-LCS: 0.22789346051514345
\end{verbatim}

\subsubsection{$\tau = 0.9$}
Sample run:
\begin{verbatim}
Example: 23/50

Reference: indus valley civilisation is known for its technological knowledge 
in a variety of fields

Ground Truth: trickle of technology reach down to the people in the different 
regions of sindh ghat
\end{verbatim}
Scores:
\begin{verbatim}
BLEU: 0.15806715806715804
ROUGE-1: 0.1595327517775904
ROUGE-2: 0.035088629933956283
ROUGE-LCS: 0.1170539764521488
\end{verbatim}

\subsection{Top-k Sampling}
Here is how Top-k sampling is implemented by selecting the greatest k logits:
\begin{verbatim}
  topk_logits, topk_indices = torch.topk(logit_last_token, self.k, dim=-1)
  topk_probs = nn.functional.softmax(topk_logits, dim=-1)
  next_token_idx = torch.multinomial(topk_probs, num_samples=1)
  next_token = topk_indices.gather(-1, next_token_idx).squeeze(-1)
\end{verbatim}
This method first selects the top-k logits (highest probabilities) from the vocabulary, 
normalizes them using softmax to form a probability distribution, and then samples the 
next token from this distribution.

\subsubsection{$k = 5$}
Sample run:
\begin{verbatim}
Example: 9/50

Reference: each city in punjab has varied preferences like people in amritsar are 
particularly fond of amritsari kulche stuffed paranthas and milk products

Ground Truth: "every city of punjab is loved by everyone just like amritsar people
are special amritsar kulche barvaan parathas and milk products"

\end{verbatim}
Scores:
\begin{verbatim}
BLEU: 0.24757281553398058
ROUGE-1: 0.24637532114470517
ROUGE-2: 0.06982793622619132
ROUGE-LCS: 0.18258561511936278
\end{verbatim}

\subsubsection{$k = 10$}
Sample run:
\begin{verbatim}
Example: 4/50

Reference: ashoka started making extensive use of stone for sculptures and great 
monuments whereas the previous tradition consisted of working with wood and clay

Ground Truth: ashok started to create statues sculptures and spectacular monuments
by using stones whereas the old traditional way is the use of wood and earth

\end{verbatim}
Scores:
\begin{verbatim}
BLEU: 0.24899274778404515
ROUGE-1: 0.2583936169635166
ROUGE-2: 0.07972418861869043
ROUGE-LCS: 0.1984233891953222
\end{verbatim}


\clearpage
\section{Task 1: Word-Constrained Decoding}
In this part, we maintain a trie data structure. We initialize the trie with the word\_list provided to us. We iterate over the word list, and for each token, generate the encoded list, and add them recursively to the trie. We do this for all the word list tokens.

Now, when we get some input token sequence, we traverse the generated tokens so far, on the trie (it will be empty in the start). Now, we only choose the indices which are present in this current trie (left after traversal). The probabilities corresponding to all other indices are set to 0 (except EOS).

Here are some sample runs:
\begin{verbatim}
Example: 1/50

completion:
Reference: an appearance is a bunch of attributes related to the service person 
like their shoes clothes tie jewellery hairstyle makeup watch cosmetics perfume etc
Ground Truth: service related to the clothes shoes jewellery makeup cosmetics perfume 
etc


Example: 2/50

completion:
Reference: ajanta located in the aurangabad district of maharashtra has twentynine 
caitya and vihara caves decorated with sculptures and paintings from the first
century bce
Ground Truth: maharashtra in aurangabad district has the ajanta the twentynine 
caves and the vihara of the first century bce and the fifth century ce 
sculptures and paint
\end{verbatim}


Here are the scores:

\begin{verbatim}
BLEU: 0.49101737747273627
ROUGE-1: 0.5166686923449297
ROUGE-2: 0.30725054217548464
ROUGE-LCS: 0.46285263939458976
\end{verbatim}


These scores, are better than all the other decoding techniques which were used in the previous task. 


We have another update on this, since the vocabulary is limited, a lot of repetition is possible. To avoid this, we can maintain a count of all the words generated so far (i.e. how many times they are generated), and reduce the probabilities of words, which have occurred more (thus enabling diversity), here are the results with this:

\begin{verbatim}
BLEU: 0.51359177384347
ROUGE-1: 0.561631318210662
ROUGE-2: 0.32122076910829656
ROUGE-LCS: 0.4811779122080838
\end{verbatim}



\end{document}
